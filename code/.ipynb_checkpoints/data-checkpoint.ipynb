{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(string):\n",
    "  \"\"\"\n",
    "  Loads list of coordinates from given string and swap out longitudes & latitudes.\n",
    "  We do the swapping because the standard is to have latitude values first, but\n",
    "  the original datasets provided in the competition have it backwards.\n",
    "  \"\"\"\n",
    "  return [(lat, long) for (long, lat) in json.loads(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_truncate(coords):\n",
    "  \"\"\"\n",
    "  Randomly truncate the end of the trip's polyline points to simulate partial trips.\n",
    "  This is only intended to be used for our custom train/validation/test datasets\n",
    "  and not for the final test dataset provided by the competition as that one is\n",
    "  already partial.\n",
    "  \"\"\"\n",
    "\n",
    "  # There's no need to truncate if there's not more than one item\n",
    "  if len(coords) <= 1:\n",
    "      return coords\n",
    "\n",
    "  # Pick a random number of items to be removed from the list.\n",
    "  # (We do \"-1\" to ensure we have at least one item left)\n",
    "  n = np.random.randint(len(coords)-1)\n",
    "  print(f\"Removing last {n} items from the trajectory\")\n",
    "\n",
    "  if n > 0:\n",
    "      # Return the list without its last n items\n",
    "      return coords[:-n]\n",
    "  else:\n",
    "      # No truncation needed in this case\n",
    "      return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_feature(feature, train, test):\n",
    "  \"\"\"\n",
    "  Encode the labels for the given feature across both the train and test datasets.\n",
    "  \"\"\"\n",
    "  encoder = LabelEncoder()\n",
    "  train_values = train[feature].copy()\n",
    "  test_values = test[feature].copy()\n",
    "  # Replace missing values with 0's so we can later encode them\n",
    "  train_values[np.isnan(train_values)] = 0\n",
    "  test_values[np.isnan(test_values)] = 0\n",
    "  # Fit the labels across all possible values in both datasets\n",
    "  encoder.fit(pd.concat([train_values, test_values]))\n",
    "  # Add new column to the datasets with encoded values\n",
    "  train[feature + '_ENCODED'] = encoder.transform(train_values)\n",
    "  test[feature + '_ENCODED'] = encoder.transform(test_values)\n",
    "  return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "  \"\"\"\n",
    "  Extract some features from the original columns in the given dataset.\n",
    "  \"\"\"\n",
    "  # Convert polyline values from strings to list objects\n",
    "  df['POLYLINE'] = df['POLYLINE'].apply(convert_coordinates)\n",
    "  # Extract start latitudes and longitudes\n",
    "  df['START_LAT'] = df['POLYLINE'].apply(lambda x: x[0][0])\n",
    "  df['START_LONG'] = df['POLYLINE'].apply(lambda x: x[0][1])\n",
    "  # Extract quarter hour of day\n",
    "  datetime_index = pd.DatetimeIndex(df['TIMESTAMP'])\n",
    "  df['QUARTER_HOUR'] = datetime_index.hour * 4 + datetime_index.minute / 15   \n",
    "  # Extract day of week\n",
    "  df['DAY_OF_WEEK'] = datetime_index.dayofweek\n",
    "  # Extract week of year\n",
    "  df['WEEK_OF_YEAR'] = datetime_index.weekofyear - 1\n",
    "  # Extract trip duration (GPS coordinates are recorded every 15 seconds)\n",
    "  df['DURATION'] = df['POLYLINE'].apply(lambda x: 15 * len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, labels):\n",
    "    \"\"\"\n",
    "    Remove some outliers that could otherwise undermine the training's results.\n",
    "    \"\"\"\n",
    "    # Remove trips that are either extremely long or short (potentially due to GPS recording issue)\n",
    "    indices = np.where((df.DURATION > 60) & (df.DURATION <= 2 * 3600))\n",
    "    df = df.iloc[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    # Remove trips that are too far away from Porto (also likely due to GPS issues)\n",
    "    bounds = (  # Bounds retrieved using http://boundingbox.klokantech.com\n",
    "        (41.052431, -8.727951),\n",
    "        (41.257678, -8.456039)\n",
    "    )\n",
    "    indices = np.where(\n",
    "        (labels[:,0]  >= bounds[0][0]) &\n",
    "        (labels[:,1] >= bounds[0][1]) &\n",
    "        (labels[:,0]  <= bounds[1][0]) &\n",
    "        (labels[:,1] <= bounds[1][1])\n",
    "    )\n",
    "    df = df.iloc[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  \"\"\"\n",
    "  Loads data from CSV files, processes and caches it in pickles for faster future loading.\n",
    "  \"\"\"\n",
    "\n",
    "  datasets = []\n",
    "  for kind in ['train', 'test']:\n",
    "    # Load original CSV file\n",
    "    csv_file = 'data/%s.csv' % kind\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"\\nListing of first few rows in {kind} dataset \\n\")\n",
    "    print(df.head(8))\n",
    "    # Ignore items that are missing data\n",
    "    df = df[df['MISSING_DATA'] == False]\n",
    "    # Ignore items that don't have polylines\n",
    "    df = df[df['POLYLINE'] != '[]']\n",
    "    # Delete the now useless column to save a bit of memory\n",
    "    df.drop('MISSING_DATA', axis=1, inplace=True)\n",
    "    # Delete an apparently useless column (all values are 'A')\n",
    "    df.drop('DAY_TYPE', axis=1, inplace=True)\n",
    "    # Fix format of timestamps\n",
    "    df['TIMESTAMP'] = df['TIMESTAMP'].astype('datetime64[s]')\n",
    "    # Extra some new features\n",
    "    extract_features(df)\n",
    "    datasets.append(df)\n",
    "  train, test = datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
